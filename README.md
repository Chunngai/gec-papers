# gec-papers

## GEC
<!-- - [x] 2021/1/6 [Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](#bert-gec) [ACL-2020] √
- [x] 2021/1/6 [GECToR - Grammatical Error Correction: Tag, Not Rewrite](#gector) [ACL-2020] √
- [x] 2021/1/7 [MaskGEC: Improving Neural Grammatical Error Correction via Dynamic Masking](#maskgec) [AAAI-2020]
- [x] 2021/1/7 [Towards Minimal Supervision BERT-Based Grammar Error Correction (Student Abstract)](#minimal-supervision) [AAAI-2020]
- [x] 2021/1/7 [Stronger Baselines for Grammatical Error Correction Using a Pretrained Encoder-Decoder Model](#bart-gec) [AACL-2020] √
- [x] 2021/1/9 [Chinese Grammatical Correction Using BERT-based Pre-trained Model](#chinese-bert-gec) [IJCNLP-2020]
- [x] 2021/1/10 [Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](#efficiency) [EMNLP-2020]
- [x] 2021/1/10 [Heterogeneous Recycle Generation for Chinese Grammatical Correction](#heterogeneous) [COLING-2020] √
- [x] 2021/1/10 [TMU-NLP System Using BERT-based Pre-trained Model to the NLP-TEA CGED Shared Task 2020](#chinese-bert-init) [AACL-2020]
- [x] 2021/1/11 [Generating Diverse Corrections with Local Beam Search for Grammatical Error Correction](#local-beam-search) [COLING-2020]
- [x] 2021/1/12 [Seq2Edits: Sequence Transduction Using Span-level Edit Operations](#seq2edits) [EMNLP-2020]
- [x] 2021/1/12 [Adversarial Grammatical Error Correction](#adversarial) [EMNLP-2020]
- [x] 2021/1/17 Pseudo-Bidirectional Decoding for Local Sequence Transduction [EMNLP-2020]
- [x] 2021/1/18 Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data [ACL-2019]
- [x] 2021/1/18 An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction [ACL-2019]
- [x] 2021/1/19 Parallel Iterative Edit Models for Local Sequence Transduction [EMNLP-2019]
- [x] 2021/1/19 Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data [NAACL-2019]
- [x] 2021/1/20 A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning [ACL-2020]
- [x] 2021/1/20 The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction [ACL-2019]
- [x] 2021/1/20 TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track [ACL-2019]
- [x] 2021/1/21 Noisy Channel for Low Resource Grammatical Error Correction [ACL-2019]
- [x] 2021/1/22 The BLCU System in the BEA 2019 Shared Task [ACL-2019]
- [x] 2021/1/22 The AIP-Tohoku System at the BEA-2019 Shared Task [ACL-2019]
- [x] 2021/1/22 CUNI System for the Building Educational Applications 2019 Shared Task: Grammatical Error Correction [ACL-2019] -->

| Index | Date | Paper | Conference | Code | Note |
| :-: | --- | --- | --- | --- | --- |
| 1* | 21/1/6 | Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction (Kaneko et al.) | ACL-2020 | [Code](https://github.com/kanekomasahiro/bert-gec) | [Note](#bert-gec) |
| 2* | 21/1/6 | GECToR - Grammatical Error Correction: Tag, Not Rewrite (Omelianchuk et al.) | ACL-2020 | [Code](https://github.com/grammarly/gector) | [Note](#gector) |
| 3* | 21/1/7 | MaskGEC: Improving Neural Grammatical Error Correction via Dynamic Masking (Zhao and Wang) | AAAI-2020 |  | [Note](#maskgec) |
| 4 | 21/1/7 | Towards Minimal Supervision BERT-Based Grammar Error Correction (Student Abstract) (Li et al.) | AAAI-2020 |  | [Note](#minimal-supervision) |
| 5* | 21/1/7 | Stronger Baselines for Grammatical Error Correction Using a Pretrained Encoder-Decoder Model (Katsumata and Komachi) | AACL-2020 | [Code](https://github.com/Katsumata420/generic-pretrained-GEC) | [Note](#bart-gec) |
| 6 | 21/1/9 | Chinese Grammatical Correction Using BERT-based Pre-trained Model (Wang et al.) | IJCNLP-2020 |  | [Note](#chinese-bert-gec) |
| 7* | 21/1/10 | Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction (Chen et al.) | EMNLP-2020 |  | [Note](#efficiency) |
| 8* | 21/1/10 | Heterogeneous Recycle Generation for Chinese Grammatical Correction (Hinson et al.) | COLING-2020 |  | [Note](#heterogeneous) |
| 9 | 21/1/10 | TMU-NLP System Using BERT-based Pre-trained Model to the NLP-TEA CGED Shared Task 2020 (Wang and Komachi) | AACL-2020 |  | [Note](#chinese-bert-init) |
| 10 | 21/1/11 | Generating Diverse Corrections with Local Beam Search for Grammatical Error Correction (Hotate et al.) | COLING-2020 |  | [Note](#local-beam-search) |
| 11 | 21/1/12 | Seq2Edits: Sequence Transduction Using Span-level Edit Operations (Stahlberg and Kumar) | EMNLP-2020 | [Code](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/transformer_seq2edits.py) | [Note](#seq2edits) |
| 12 | 21/1/12 | Adversarial Grammatical Error Correction (Raheja and Alikaniotis) | EMNLP-2020 |  | [Note](#adversarial) |
| 13 | 21/1/17 | Pseudo-Bidirectional Decoding for Local Sequence Transduction (Zhou et al.) | EMNLP-2020 |  |  |
| 14 | 21/1/18 | Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data (Grundkiewicz et al.) | ACL-2019 |  |  |
| 15 | 21/1/18 | An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction (Kiyono et al.) | ACL-2019 |  |  |
| 16 | 21/1/19 | Parallel Iterative Edit Models for Local Sequence Transduction (Awasthi et al.) | EMNLP-2019 |  |  |
| 17 | 21/1/19 | Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data (Zhao et al.) | NAACL-2019 |  |  |
| 18 | 21/1/20 | A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning (Choe et al.) | ACL-2020 |  |  |
| 19 | 21/1/20 | The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction (Alikaniotis and Raheja) | ACL-2019 |  |  |
| 20 | 21/1/20 | TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track (Kaneko et al.) | ACL-2019 |  |  |
| 21 | 21/1/21 | Noisy Channel for Low Resource Grammatical Error Correction (Flachs et al.) | ACL-2019 |  |  |
| 22 | 21/1/22 | The BLCU System in the BEA 2019 Shared Task (Yang et al.) | ACL-2019 |  |  |
| 23 | 21/1/22 | The AIP-Tohoku System at the BEA-2019 Shared Task (Asano et al.) | ACL-2019 |  |  |
| 24 | 21/1/22 | CUNI System for the Building Educational Applications 2019 Shared Task: Grammatical Error Correction (Náplava and Straka) | ACL-2019 |  |  |
| 25 | 21/1/27 | Cross-Sentence Grammatical Error Correction (Chollampatt et al.) | ACL-2019 |  |  |

## GED
<!-- - [x] 2021/1/6 [基于数据增强和多任务特征学习的中文语法错误检测方法](#chinese-multi-task) [CCL-2020] √
- [x] 2021/1/11 [Integrating BERT and Score-based Feature Gates for Chinese Grammatical Error Diagnosis](#score-based) [AACL-2020]
- [x] 2021/1/11 [CYUT Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2020 CGED Shared](#bert-crf) [AACL-2020]
- [x] 2021/1/11 [Combining ResNet and Transformer for Chinese Grammatical Error Diagnosis](#resnet-bert) [AACL-2020]
- [x] 2021/1/11 [Chinese Grammatical Errors Diagnosis System Based on BERT at NLPTEA-2020 CGED Shared Task](#bert-bilstm-crf-3gram-seq2seq) [AACL-2020]
- [x] 2021/1/11 [Chinese Grammatical Error Detection Based on BERT Model](#bert-finetuned) [AACL-2020]
- [x] 2021/1/21 Multi-Head Multi-Layer Attention to Deep Language Representations for Grammatical Error Detection [CICLING-2019] -->

| Index | Date | Paper | Conference | Code | Note |
| :-: | --- | --- | --- | --- | --- |
| 1* | 21/1/6 | 基于数据增强和多任务特征学习的中文语法错误检测方法 (Xie et al.) | CCL-2020 |  | [Note](#chinese-multi-task) |
| 2 | 21/1/11 | Integrating BERT and Score-based Feature Gates for Chinese Grammatical Error Diagnosis (Cao et al.) | AACL-2020 |  | [Note](#score-based) |
| 3 | 21/1/11 | CYUT Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2020 CGED Shared (Wu and Wang) | AACL-2020 |  | [Note](#bert-crf) |
| 4 | 21/1/11 | Combining ResNet and Transformer for Chinese Grammatical Error Diagnosis (Wang et al.) | AACL-2020 |  | [Note](#resnet-bert) |
| 5 | 21/1/11 | Chinese Grammatical Errors Diagnosis System Based on BERT at NLPTEA-2020 CGED Shared Task (Zan et al.) | AACL-2020 |  | [Note](#bert-bilstm-crf-3gram-seq2seq) |
| 6 | 21/1/11 | Chinese Grammatical Error Detection Based on BERT Model (Cheng and Duan) | AACL-2020 |  | [Note](#bert-finetuned) |
| 7 | 21/1/21 | Multi-Head Multi-Layer Attention to Deep Language Representations for Grammatical Error Detection (Kaneko et al.) | CICLING-2019 |  |  |

## DA
<!-- - [x] Improving Grammatical Error Correction with Machine Translation Pairs [EMNLP-2020]
- [x] A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction [EMNLP-2020] -->

| Index | Date | Paper | Conference | Code | Note |
| :-: | --- | --- | --- | --- | --- |
| 1 | 21/1/11 | A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction (Mita et al.) | EMNLP-2020 |  |  |

## Related
<!-- - [x] 2021/1/5 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- [x] 2021/1/5 [Incorporating BERT into Neural Machine Translation](#bert-nmt) [ICLR-2020] √
- [x] 2021/1/17 Agreement on Target-Bidirectional LSTMs for Sequence-to-Sequence Learning [AAAI-2016]
- [x] 2021/1/17 Agreement on Target-bidirectional Neural Machine Translation [NAACL-2016]
- [x] 2021/1/17 Edinburgh Neural Machine Translation Systems for WMT 16
- [x] 2021/1/22 LIMIT-BERT: Linguistic Informed Multi-Task BERT [EMNLP-2020]
- [x] 2021/1/23 Distilling Knowledge Learned in BERT for Text Generation [ACL-2020]
- [x] 2021/1/23 Towards Making the Most of BERT in Neural Machine Translation [AAAI-2020]
- [x] 2021/1/23 Acquiring Knowledge from Pre-Trained Model to Neural Machine Translation [AAAI-2020] -->

| Index | Date | Paper | Conference | Code | Note |
| :-: | --- | --- | --- | --- | --- |
| 1 | 21/1/5 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al.) | NAACL-2019 |  |  |
| 2* | 21/1/5 | Incorporating BERT into Neural Machine Translation (Zhu et al.) | ICLR-2020 | [Code](https://github.com/bert-nmt/bert-nmt) | [Note](#bert-nmt) |
| 3 | 21/1/17 | Agreement on Target-Bidirectional LSTMs for Sequence-to-Sequence Learning (Liu et al.) | AAAI-2016 |  |  |
| 4 | 21/1/17 | Agreement on Target-bidirectional Neural Machine Translation (Liu et al.) | NAACL-2016 |  |  |
| 5* | 21/1/17 | Edinburgh Neural Machine Translation Systems for WMT 16 (Sennrich et al.) | WMT-2016 |  |  |
| 6 | 21/1/22 | LIMIT-BERT: Linguistic Informed Multi-Task BERT (Zhou et al.) | EMNLP-2020 |  |  |
| 7 | 21/1/23 | Distilling Knowledge Learned in BERT for Text Generation (Chen et al.) | ACL-2020 |  |  |
| 8 | 21/1/23 | Towards Making the Most of BERT in Neural Machine Translation (Yang et al.) | AAAI-2020 |  |  |
| 9 | 21/1/23 | Acquiring Knowledge from Pre-Trained Model to Neural Machine Translation (Weng et al.) | AAAI-2020 |  |  |
| 10 | 21/1/26 | Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting (Zhou et al.) | - |  |  |

---

## Seq2Seq
1. <span id="bert-gec">[ACL-2020] Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction</span>  
Applied the BERT-fused model for GEC. The BERT is finetuned with MLM and GED to fix the inconsistent input distribution between the raw data for BERT training and the GEC data. Pseudo-data and R2L are also used for performance boosting.  
https://github.com/kanekomasahiro/bert-gec

2. <span id="chinese-bert-gec">[IJCNLP-2020] Chinese Grammatical Correction Using BERT-based Pre-trained Model  
Tries BERT-init (BERT-encoder in the papar) and BERT-fused for Chinese GEC. The Chinese GEC ver. of *Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction*, even less techniques used.

3. <span id="bart-gec">[AACL-2020] Stronger Baselines for Grammatical Error Correction Using a Pretrained Encoder-Decoder Model</span>  
Used BART for GEC and says that BART can be a baseline for GEC, which can reach high performance by simple finetuning with GEC data instead of pseudo-data pretraining.  
https://github.com/Katsumata420/generic-pretrained-GEC

4. <span id="efficiency">[EMNLP-2020] Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction</span>  
Combines a sequence tagging model for erroneous span detection and a seq2seq model for erroneous span correction to make the GEC process more efficient. The sequence tagging model (BERT-like) looks for spans needing to be corrected by outputting binary vectors, and the seq2seq model receives inputs annotated according to the outputs of the sequence tagging model and only produces outputs corresponding to the detected spans. Pseudo-data is used for pre-training the ESD and ESC models.

## Seq2Edits
1. <span id="seq2edits">[EMNLP-2020] Seq2Edits: Sequence Transduction Using Span-level Edit Operations</span>  
Proposes a method for tasks containing many overlaps such as GEC. Uses Transformer with the decoder modified. The model receives a source sentence and at each inference time-step outputs a 3-tuple  which corresponds to an edit operation (error tag, source span end position, replacement). The error tag provides clear explanation. The paper conducts experiments on 5 NLP tasks containing many overlaps. Experiments with and without pretraining are conducted.  
(Not very clear about the modified decoder.)  
https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/transformer_seq2edits.py

## Seq Labeling
1. <span id="gector">[ACL-2020] GECToR - Grammatical Error Correction: Tag, Not Rewrite</span>  
Used a BERT sequence tagger. Developed custom task-specific g-transformations such as CASE, MERGE and so on. Since each time a token in the source sentence can only map an edit, iterative correction may be required. A 3-stage training strategy is used: data-aug pretraining - finetuning on err data - finetuning on err and err-free data.  
https://github.com/grammarly/gector

2. <span id="minimal-supervision">[AAAI-2020] Towards Minimal Supervision BERT-Based Grammar Error Correction (Student Abstract)</span>  
Divides the GEC task into two stages: error identification and error correction. The first stage is a sequence tagging (remain, substitution, ...) task and a BERT is used for the second stage (correction).   
(Not very clear about the method proposed by the paper.)

## Pipeline
1. <span id="heterogeneous">[COLING-2020] Heterogeneous Recycle Generation for Chinese Grammatical Correction</span>  
Makes use of a sequence editing model, a seq2seq model and a spell checker to correct different kinds of errors (small scale errors, large scale errors and spell errors respectively). Iterative decoding is applied on (sequence editing model, seq2seq model). The proposed method needs not data-aug but still achieves comparable performance.

## Multi-Task Learning
1. <span id="chinese-multi-task">[GED] [CCL-2020] 基于数据增强和多任务特征学习的中文语法错误检测方法</span>  
Implements Chinese GED through data-augmentation and pretrained BERT finetuned using multi-task learning. The data-augmentation method applied here is simple, including manipulations such as insertions, deletions and so on. Some rules are designed to maintain the meanings of sentences. The Chinese BERT is used for GED with a CRF layer on top. It is finetuned through multi-task learning: pos tagging, parsing and grammar error detection.

## Beam Search
1. <span id="local-beam-search">[COLING-2020] Generating Diverse Corrections with Local Beam Search for Grammatical Error Correction</span>  
Proposes a local beam search method to output diverse outputs. The proposed method generates more diverse outputs than the plain beam search, and only modifies where should be corrected rather than changing the whole sequence as the global beam search. The copy factor in the copy-augmented Transformer is used as a penalty score.

## Adversarial Training
1. <span id="adversarial">[EMNLP-2020] Adversarial Grammatical Error Correction</span>  
The first approach to use adversarial training for GEC. Uses a seq2seq model as the generator and a sentence-pair classification model for the discriminator. The discriminator basically acts as a novel evaluation method for evaluating the outputs generated by the generator, which directly models the task. No other technique such as data augmentation is used.  
(Not very clear about the adversarial training.)

## Dynamic Masking
1. <span id="maskgec">[AAAI-2020] MaskGEC: Improving Neural Grammatical Error Correction via Dynamic Masking</span>  
Proposed a dynamic masking method for data-augmentation and generalization boosting. In each epoch each sentence is introduced noises with a prob by some manipulations, including padding substitution, random substution, word frequency substitution and so on.

## NLPTEA
1. <span id="chinese-bert-init">[AACL-2020] TMU-NLP System Using BERT-based Pre-trained Model to the NLP-TEA CGED Shared Task 2020  
Uses BERT-init as in *Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction*, which is also the same as the BERT-encoder in *Chinese Grammatical Correction Using BERT-based Pre-trained Model*.

2. <span id="score-based">[GED] [AACL-2020] Integrating BERT and Score-based Feature Gates for Chinese Grammatical Error Diagnosis</span>  
Uses BiLSTM-CRF for GED, whose input is features concat composed of output of BERT, POS, POS score and PMI score. The scores are incorporated using a gating mechanism to avoid losing partial-order relationships when embedding continuous feature items.  
(Not very clear about the features used and the purpose of the gating mechanism.)

3. <span id="bert-crf">[GED] [AACL-2020] CYUT Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2020 CGED Shared]</span>  
Uses BERT + CRF.

4. <span id="resnet-bert">[GED] [AACL-2020] Combining ResNet and Transformer for Chinese Grammatical Error Diagnosis</span>  
Applies res on BERT for GED. The encoded hidden repr is added with the emd and fed into the output layer.  
(Also related to GEC but not detailed, thus catogorize as GED.)

5. <span id="bert-bilstm-crf-3gram-seq2seq">[GED] [AACL-2020] Chinese Grammatical Errors Diagnosis System Based on BERT at NLPTEA-2020 CGED Shared Task</span>  
Uses BERT-BiLSTM-CRF for GED. Uses a hybrid system containing a 3-gram and a seq2seq for GEC.

6. <span id="bert-finetuned">[GED] [AACL-2020] Chinese Grammatical Error Detection Based on BERT Model</span>  
Uses BERT finetuned on GEC datasets.

## Related
1. <span id="bert-nmt">[NMT] [ICLR-2020] Incorporating BERT into Neural Machine Translation</span>  
Proposed a BERT-fused model. Comparing with the Vanilla Transformer, the proposed model has additionally one BERT-Enc Attention module in the encoder and a BERT-Dec Attention module in the decoder. Both of the additional modules are for incorporating features extracted by BERT whose weights are fixed. A Vanilla Transformer is trained in the first training stage, and in the second stage the BERT and additional modules are trained together.  
https://github.com/bert-nmt/bert-nmt
